{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import typing\n",
    "from typing import List\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from cleo.cleoCLAP import CLEOClap\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import ClapProcessor, ClapModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from cleo.QFormer import BertConfig, BertLMHeadModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "clapModelVr = \"laion/clap-htsat-unfused\"\n",
    "dataset = load_dataset(\"patrickvonplaten/librispeech_asr_self_contained\", split=\"train.clean.100\")\n",
    "audio_gpu = \"cpu\"\n",
    "clapModelProcessor = ClapProcessor.from_pretrained(clapModelVr)\n",
    "clapModel = ClapModel.from_pretrained(clapModelVr)\n",
    "clapModel = clapModel.to(audio_gpu)\n",
    "\n",
    "class CLEODataset(Dataset):\n",
    "    def __init__(self, dataset, instruction, processor, sampling_rate = 48000):\n",
    "        self.dataset = dataset\n",
    "        self.instruction = instruction\n",
    "        self.processor = processor\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ## Create the label\n",
    "        label = self.dataset[idx][\"text\"].lower()\n",
    "        \n",
    "        ## Save the audio\n",
    "        audio_array = self.dataset[idx][\"audio\"][\"array\"]\n",
    "        return self.instruction, audio_array, label\n",
    "\n",
    "def custom_collate_fn(original_batch):\n",
    "    instructions = [each[0] for each in original_batch]\n",
    "    audios = [each[1] for each in original_batch]\n",
    "    labels = [each[2] for each in original_batch]\n",
    "    return instructions, audios, labels\n",
    "\n",
    "instruction = \"\"\"Repeat back the information that you see below:\n",
    "<wav>\n",
    "\n",
    "Information:\n",
    "\"\"\"\n",
    "cleoDataset = CLEODataset(dataset, instruction, clapModelProcessor)\n",
    "train_dataloader = DataLoader(cleoDataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "batch_idx, (instructions, audios, labels) = next(enumerate(train_dataloader))\n",
    "print(\"Dataset Loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2eeeba05ea4de78820bcffaaeb12b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded\n"
     ]
    }
   ],
   "source": [
    "## Initialize QFormer\n",
    "def init_Qformer(num_query_token, audio_width, freeze):\n",
    "    encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "    encoder_config.encoder_width = audio_width\n",
    "    # insert cross-attention layer every other block\n",
    "    encoder_config.add_cross_attention = True\n",
    "    encoder_config.cross_attention_freq = 2\n",
    "    encoder_config.query_length = num_query_token\n",
    "    Qformer = BertLMHeadModel(config=encoder_config)\n",
    "    query_tokens = nn.Parameter(\n",
    "        torch.zeros(1, num_query_token, encoder_config.hidden_size)\n",
    "    )\n",
    "    query_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\n",
    "\n",
    "#    Qformer.cls = None\n",
    "#    Qformer.bert.embeddings.word_embeddings = None\n",
    "#    Qformer.bert.embeddings.position_embeddings = None\n",
    "#    for layer in Qformer.bert.encoder.layer:\n",
    "#        layer.output = None\n",
    "#        layer.intermediate = None\n",
    "\n",
    "    if freeze:\n",
    "        for name, param in Qformer.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        Qformer = Qformer.eval()\n",
    "        query_tokens.requires_grad = False\n",
    "        logging.info(\"freeze Qformer\")\n",
    "    return Qformer, query_tokens\n",
    "\n",
    "def __load_llm__(llm_model, freeze_llm, pad_token_id=None, device=\"cpu\"):\n",
    "    ## Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "    if pad_token_id is not None:\n",
    "        tokenizer.pad_token_id = pad_token_id\n",
    "    else:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(llm_model, device_map=device)\n",
    "    logging.info(\"Loaded LLAMA model\")\n",
    "    if freeze_llm:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        logging.info(\"Model parameters frozen\")\n",
    "    return tokenizer, model\n",
    "\n",
    "## Load the qformer model\n",
    "num_query_tokens = 32\n",
    "audio_width = 512\n",
    "freeze = False\n",
    "Qformer, query_tokens = init_Qformer(num_query_tokens, audio_width, freeze)\n",
    "Qformer = Qformer.to(audio_gpu)\n",
    "query_tokens = query_tokens.to(audio_gpu)\n",
    "\n",
    "## Load the LLM model\n",
    "tokenizer, llm = __load_llm__(\"/home/models/Llama-2-7b-hf\", True, device=\"cpu\")\n",
    "\n",
    "## Create projection layer\n",
    "proj = nn.Linear(Qformer.config.hidden_size, llm.config.hidden_size)\n",
    "proj = proj.to(audio_gpu)\n",
    "print(\"All models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Def get audio embeddings\n",
    "def get_audio_embeddings(audios):\n",
    "    inputs = clapModelProcessor(audios=audios, sampling_rate=48000, return_tensors=\"pt\")\n",
    "    if audio_gpu != \"cpu\":\n",
    "        inputs = inputs.to(audio_gpu)\n",
    "    with torch.no_grad():\n",
    "        embeddings = clapModel.get_audio_features(**inputs, return_dict=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def encode_audio(audios):\n",
    "    ## Get the embeddings first\n",
    "    wav_embs = get_audio_embeddings(audios)\n",
    "    wav_embs = wav_embs.unsqueeze(1)\n",
    "    if audio_gpu != \"cpu\":\n",
    "        wav_embs = wav_embs.to(audio_gpu)\n",
    "\n",
    "    ## Create the attention mask for the wav\n",
    "    wav_attn = torch.ones(wav_embs.size()[:-1], dtype=torch.long).to(audio_gpu)\n",
    "\n",
    "    ## Expand the query tokens\n",
    "    wav_query_tokens = query_tokens.expand(wav_embs.shape[0], -1, -1)\n",
    "\n",
    "    ## Create Qformer output\n",
    "    query_output = Qformer.bert(\n",
    "        query_embeds = wav_query_tokens,\n",
    "        encoder_hidden_states = wav_embs,\n",
    "        encoder_attention_mask = wav_attn,\n",
    "        return_dict = True\n",
    "    )\n",
    "\n",
    "    return query_output\n",
    "\n",
    "def project_query(query_output):\n",
    "    wav_input = proj(query_output[\"last_hidden_state\"])\n",
    "    wav_attn = torch.ones(wav_input.size()[:-1], dtype=torch.long).to(audio_gpu)\n",
    "    return wav_input, wav_attn\n",
    "\n",
    "def encode_text(text, device=\"cpu\"):\n",
    "    output_dict = llm(tokenizer.encode(text, return_tensors=\"pt\").to(device), return_dict=True, output_hidden_states=True)\n",
    "    return output_dict\n",
    "\n",
    "def ATC(audios, labels):\n",
    "    ## Get the wav_input and wav_attn\n",
    "    wav_rep, _ = project_query(encode_audio(audios))\n",
    "    wav_rep = wav_rep[:,-1,:]\n",
    "\n",
    "    ## Get the text_input\n",
    "    text_rep = []\n",
    "    for label in labels:\n",
    "        text_rep.append(encode_text(label, device=\"cuda:1\").hidden_states[-1][:,-1,:])\n",
    "    text_rep = torch.cat(text_rep, dim=0)    \n",
    "\n",
    "    temp = .5\n",
    "    similarity = torch.matmul(wav_rep, text_rep.T) * temp\n",
    "    labels = torch.arange(similarity.shape[0], device=similarity.device, dtype=torch.long)\n",
    "    loss = (\n",
    "        F.cross_entropy(similarity, labels, reduction=\"mean\")\n",
    "        + F.cross_entropy(similarity.T, labels, reduction=\"mean\")\n",
    "    ) / 2\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the wav_input and wav_attn\n",
    "wav_embs = get_audio_embeddings(audios)\n",
    "wav_embs = wav_embs.unsqueeze(1)\n",
    "\n",
    "## Create the attention mask for the wav\n",
    "wav_attn = torch.ones(wav_embs.size()[:-1], dtype=torch.long).to(audio_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from info_nce import InfoNCE, info_nce\n",
    "## Expand the query tokens\n",
    "wav_query_tokens = query_tokens.expand(wav_embs.shape[0], -1, -1)\n",
    "\n",
    "## Create Qformer output\n",
    "wav_output = Qformer.bert(\n",
    "    query_embeds = wav_query_tokens,\n",
    "    encoder_hidden_states = wav_embs,\n",
    "    encoder_attention_mask = wav_attn,\n",
    "    return_dict = True\n",
    ")\n",
    "\n",
    "Qtokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "stuff = Qtokenizer(labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "text_output = Qformer.bert(\n",
    "    input_ids = stuff[\"input_ids\"],\n",
    "    attention_mask = stuff[\"attention_mask\"],\n",
    "    return_dict = True\n",
    ")\n",
    "\n",
    "best_match = torch.argmax(F.cosine_similarity(wav_output.last_hidden_state[:,:,:], text_output.last_hidden_state[:,0,:].unsqueeze(1), dim=2), dim=1)\n",
    "wav_rep = torch.gather(wav_output.last_hidden_state, 1, best_match.view(-1, 1, 1).expand(-1, 1, 768)).squeeze(1)\n",
    "cls_rep = text_output.last_hidden_state[:,0,:]\n",
    "loss = InfoNCE()\n",
    "output = loss(wav_rep, cls_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, embedding_size = 32, 128\n",
    "query = torch.randn(batch_size, embedding_size)\n",
    "positive_key = torch.randn(batch_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_rep = torch.gather(wav_output.last_hidden_state, 1, best_match.view(-1, 1, 1).expand(-1, 1, 768)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loss(wav_rep, cls_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8851, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
